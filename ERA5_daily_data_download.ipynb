{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T01:42:06.816317200Z",
     "start_time": "2025-07-03T01:41:55.262834800Z"
    }
   },
   "outputs": [],
   "source": [
    "#数据准备\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import netCDF4 as nc\n",
    "import pandas as pd\n",
    "import math\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.mpl.ticker as cticker\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from metpy import units\n",
    "from netCDF4 import Dataset\n",
    "import metpy.constants as constants\n",
    "import metpy.calc as mpcalc\n",
    "import cartopy.io.shapereader as shpreader\n",
    "from matplotlib.path import Path\n",
    "from cartopy.mpl.patch import geos_to_path\n",
    "from metpy.interpolate import inverse_distance_to_grid\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.interpolate import Rbf\n",
    "import scipy.ndimage as ndimage\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import time\n",
    "from matplotlib.dates import DateFormatter\n",
    "import cftime\n",
    "import mpath\n",
    "import matplotlib.path as mpath\n",
    "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
    "import os\n",
    "import glob\n",
    "import dask.array as da\n",
    "import cmaps\n",
    "import import_ipynb\n",
    "import code_study as cs\n",
    "# %run code_study.ipynb\n",
    "# from _future_ import print_function\n",
    "import torch\n",
    "import code_study as cs\n",
    "from datetime import datetime as dt\n",
    "import cdsapi"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def time_before_after(date_str):\n",
    "    from datetime import datetime, timedelta\n",
    "\n",
    "    # 将字符串转换为日期对象\n",
    "    date_obj = datetime.strptime(date_str, '%Y%m%d')\n",
    "    \n",
    "    # 计算前一天的日期\n",
    "    previous_day = date_obj - timedelta(days=1)\n",
    "    # if str(date_str)[4:]=='0101':\n",
    "    #     previous_day = date_obj + timedelta(days=0)    \n",
    "    # 计算后一天的日期\n",
    "    next_day = date_obj + timedelta(days=1)\n",
    "    # if str(date_str)[4:]=='0228':\n",
    "    #     next_day = date_obj + timedelta(days=0) \n",
    "    # 将日期对象格式化为字符串\n",
    "    previous_day_str = previous_day.strftime('%Y%m%d')\n",
    "    next_day_str = next_day.strftime('%Y%m%d')\n",
    "    if str(date_str)[4:]=='0101':\n",
    "        return date_str,next_day_str\n",
    "    if str(date_str)[4:]=='0228':\n",
    "        return previous_day_str,date_str\n",
    "    else:\n",
    "        return previous_day_str,date_str,next_day_str"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-03T01:42:06.828746100Z",
     "start_time": "2025-07-03T01:42:06.817319500Z"
    }
   },
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def data_download(var,date):\n",
    "    c = cdsapi.Client()\n",
    "    \n",
    "    # The directory for forecastsd\n",
    "    ## Use os.path.join to give cross platform compatibility\n",
    "    \n",
    "    # The variables required\n",
    "    variables_all = ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'mean_sea_level_pressure','geopotential', 'temperature', 'u_component_of_wind', 'v_component_of_wind', \"relative_humidity\"]\n",
    "    var_all=['T2m', 'U10', 'V10', 'MSL','Z', 'T', 'U', 'V', 'R']\n",
    "    var_index=var_all.index(var)\n",
    "    # Area to download\n",
    "    area = [90, 0, -90, 359]\n",
    "    \n",
    "    # Pressure levels required\n",
    "    pressure_levels = [50, 100, 150, 200, 250, 300, 400, 500, 600, 700, 850, 925, 1000]\n",
    "    \n",
    "    # Download the surface data\n",
    "    c.retrieve('reanalysis-era5-single-levels', {\n",
    "        'product_type': 'reanalysis',\n",
    "        'format': 'netcdf',\n",
    "        'variable': variables_all[var_index],\n",
    "        'date': date,\n",
    "        'time': [\n",
    "            \"00:00\",\n",
    "            \"06:00\",\n",
    "            \"12:00\",\n",
    "            \"18:00\",\n",
    "        ],\n",
    "        'area': area,\n",
    "        \"grid\": \"1.0/1.0\",\n",
    "    },  (\"E:\\data\\ERA5.daily.1x1\\\\\"+var_all[var_index]+\"\\Hourly\\\\\"+ var_all[var_index]+date+'.nc'))\n",
    "    \n",
    "    # # Download the upper air data\n",
    "    # c.retrieve('reanalysis-era5-pressure-levels', {\n",
    "    #     'product_type': 'reanalysis',\n",
    "    #     'format': 'netcdf',\n",
    "    #     'variable': variables_all[var_index],\n",
    "    #     'pressure_level': pressure_levels,\n",
    "    #     'date': date,\n",
    "    #     'time': [\n",
    "    #         \"00:00\",\n",
    "    #         \"06:00\",\n",
    "    #         \"12:00\",\n",
    "    #         \"18:00\",\n",
    "    #     ],\n",
    "    #     'area': area,\n",
    "    #     \"grid\": \"1.0/1.0\",\n",
    "    # },  (\"E:\\data\\ERA5.daily.1x1\\\\\"+var_all[var_index]+\"\\Hourly\\\\\"+ var_all[var_index]+date+'.nc'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-03T01:42:06.851231100Z",
     "start_time": "2025-07-03T01:42:06.831746600Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "date='20231201'\n",
    "var='T2m'\n",
    "for i in range(61+366+92):\n",
    "    date_following = pd.to_datetime(date) + pd.Timedelta(days=i)\n",
    "    date_str = date_following.strftime('%Y%m%d')\n",
    "    if os.path.exists(r'E:\\data\\ERA5.daily.1x1\\\\'+var+'\\\\Hourly\\\\'+var+date_str+'.nc') == True:\n",
    "        continue\n",
    "    data_download(var,date_str)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "date='20231201'\n",
    "var='Z'\n",
    "for i in range(61+366+92):\n",
    "    date_following = pd.to_datetime(date) + pd.Timedelta(days=i)\n",
    "    date_str = date_following.strftime('%Y%m%d')\n",
    "    if os.path.exists(r'E:\\data\\ERA5.daily.1x1\\\\'+var+'\\\\Hourly\\\\'+var+date_str+'.nc') == True:\n",
    "        continue\n",
    "    data_download(var,date_str)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "var='T2m'\n",
    "years = [ str(id1) for id1 in range(2023,2024) ]\n",
    "months =[ '%02d' % id2 for id2 in range(12,13) ]\n",
    "for yr in years:\n",
    "    for mn in months:\n",
    "        list=glob.glob(r'E:\\data\\ERA5.daily.1x1\\\\'+var+'\\\\Hourly\\\\'+var+yr+mn+'*.nc')\n",
    "        list.sort()\n",
    "        data=[]\n",
    "        for i in range(len(list)):\n",
    "            data_sa=(xr.open_dataset(list[i])[var.lower()].transpose('valid_time','latitude','longitude'))\n",
    "            data.append(data_sa)\n",
    "        dataal=xr.concat(data,dim='valid_time')\n",
    "        data_daily=dataal.rolling(valid_time=4).mean().sel(valid_time=dataal.valid_time[3::4])\n",
    "        start_dt = f\"{yr}-{mn}-01\"\n",
    "        end_dt = pd.Timestamp(start_dt) + pd.offsets.MonthEnd(1)\n",
    "        date_var=pd.date_range(start_dt, end_dt, freq='D').to_numpy()\n",
    "        data=xr.DataArray(data_daily,coords=[date_var,dataal.pressure_level,dataal.latitude,dataal.longitude],dims=['initial_time0_hours','g0_lat_1','g0_lon_2'],name=var)\n",
    "        data.to_netcdf('E:\\\\data\\\\ERA5.daily.1x1\\\\'+var+'\\\\'+var+'.'+yr+mn+'.nc')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "var='T'\n",
    "years = [ str(id1) for id1 in range(2024,2026) ]\n",
    "months =[ '%02d' % id2 for id2 in range(1,13) ]\n",
    "for yr in years:\n",
    "    for mn in months:\n",
    "        list=glob.glob(r'E:\\data\\ERA5.daily.1x1\\\\'+var+'\\\\Hourly\\\\'+var+yr+mn+'*.nc')\n",
    "        list.sort()\n",
    "        data=[]\n",
    "        for i in range(len(list)):\n",
    "            data_sa=(xr.open_dataset(list[i])[var.lower()].transpose('valid_time','pressure_level','latitude','longitude'))[:,:,::-1,:].sortby('pressure_level')\n",
    "            data.append(data_sa)\n",
    "        dataal=xr.concat(data,dim='valid_time')\n",
    "        data_daily=dataal.rolling(valid_time=4).mean().sel(valid_time=dataal.valid_time[3::4])\n",
    "        start_dt = f\"{yr}-{mn}-01\"\n",
    "        end_dt = pd.Timestamp(start_dt) + pd.offsets.MonthEnd(1)\n",
    "        date_var=pd.date_range(start_dt, end_dt, freq='D').to_numpy()\n",
    "        data=xr.DataArray(data_daily,coords=[date_var,dataal.pressure_level,dataal.latitude,dataal.longitude],dims=['initial_time0_hours','lv_ISBL1','g0_lat_2','g0_lon_3'],name=var)\n",
    "        data.to_netcdf('E:\\\\data\\\\ERA5.daily.1x1\\\\'+var+'\\\\'+var+'.'+yr+mn+'.nc')"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "must supply at least one object to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mStopIteration\u001B[0m                             Traceback (most recent call last)",
      "File \u001B[1;32mC:\\Python\\anaconda\\lib\\site-packages\\xarray\\core\\concat.py:254\u001B[0m, in \u001B[0;36mconcat\u001B[1;34m(objs, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs, create_index_for_new_dim)\u001B[0m\n\u001B[0;32m    253\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 254\u001B[0m     first_obj, objs \u001B[38;5;241m=\u001B[39m \u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpeek_at\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobjs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    255\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32mC:\\Python\\anaconda\\lib\\site-packages\\xarray\\core\\utils.py:199\u001B[0m, in \u001B[0;36mpeek_at\u001B[1;34m(iterable)\u001B[0m\n\u001B[0;32m    198\u001B[0m gen \u001B[38;5;241m=\u001B[39m \u001B[38;5;28miter\u001B[39m(iterable)\n\u001B[1;32m--> 199\u001B[0m peek \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mgen\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    200\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m peek, itertools\u001B[38;5;241m.\u001B[39mchain([peek], gen)\n",
      "\u001B[1;31mStopIteration\u001B[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 12\u001B[0m\n\u001B[0;32m     10\u001B[0m     data_sa\u001B[38;5;241m=\u001B[39m(xr\u001B[38;5;241m.\u001B[39mopen_dataset(\u001B[38;5;28mlist\u001B[39m[i])[var\u001B[38;5;241m.\u001B[39mlower()]\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalid_time\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpressure_level\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlatitude\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlongitude\u001B[39m\u001B[38;5;124m'\u001B[39m))[:,:,::\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m,:]\u001B[38;5;241m.\u001B[39msortby(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpressure_level\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     11\u001B[0m     data\u001B[38;5;241m.\u001B[39mappend(data_sa)\n\u001B[1;32m---> 12\u001B[0m dataal\u001B[38;5;241m=\u001B[39m\u001B[43mxr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconcat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mvalid_time\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m data_daily\u001B[38;5;241m=\u001B[39mdataal\u001B[38;5;241m.\u001B[39mrolling(valid_time\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m)\u001B[38;5;241m.\u001B[39mmean()\u001B[38;5;241m.\u001B[39msel(valid_time\u001B[38;5;241m=\u001B[39mdataal\u001B[38;5;241m.\u001B[39mvalid_time[\u001B[38;5;241m3\u001B[39m::\u001B[38;5;241m4\u001B[39m])\n\u001B[0;32m     14\u001B[0m start_dt \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00myr\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m-\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmn\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m-01\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32mC:\\Python\\anaconda\\lib\\site-packages\\xarray\\core\\concat.py:256\u001B[0m, in \u001B[0;36mconcat\u001B[1;34m(objs, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs, create_index_for_new_dim)\u001B[0m\n\u001B[0;32m    254\u001B[0m     first_obj, objs \u001B[38;5;241m=\u001B[39m utils\u001B[38;5;241m.\u001B[39mpeek_at(objs)\n\u001B[0;32m    255\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m--> 256\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmust supply at least one object to concatenate\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m    258\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m compat \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mset\u001B[39m(_VALID_COMPAT) \u001B[38;5;241m-\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mminimal\u001B[39m\u001B[38;5;124m\"\u001B[39m}:\n\u001B[0;32m    259\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    260\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcompat=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcompat\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m invalid: must be \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbroadcast_equals\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mequals\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124midentical\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mno_conflicts\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m or \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moverride\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    261\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: must supply at least one object to concatenate"
     ]
    }
   ],
   "source": [
    "var='Z'\n",
    "years = [ str(id1) for id1 in range(2024,2026) ]\n",
    "months =[ '%02d' % id2 for id2 in range(1,13) ]\n",
    "for yr in years:\n",
    "    for mn in months:\n",
    "        list=glob.glob(r'E:\\data\\ERA5.daily.1x1\\\\'+var+'\\\\Hourly\\\\'+var+yr+mn+'*.nc')\n",
    "        list.sort()\n",
    "        data=[]\n",
    "        for i in range(len(list)):\n",
    "            data_sa=(xr.open_dataset(list[i])[var.lower()].transpose('valid_time','pressure_level','latitude','longitude'))[:,:,::-1,:].sortby('pressure_level')\n",
    "            data.append(data_sa)\n",
    "        dataal=xr.concat(data,dim='valid_time')\n",
    "        data_daily=dataal.rolling(valid_time=4).mean().sel(valid_time=dataal.valid_time[3::4])\n",
    "        start_dt = f\"{yr}-{mn}-01\"\n",
    "        end_dt = pd.Timestamp(start_dt) + pd.offsets.MonthEnd(1)\n",
    "        date_var=pd.date_range(start_dt, end_dt, freq='D').to_numpy()\n",
    "        data=xr.DataArray(data_daily,coords=[date_var,dataal.pressure_level,dataal.latitude,dataal.longitude],dims=['initial_time0_hours','lv_ISBL1','g0_lat_2','g0_lon_3'],name=var)\n",
    "        data.to_netcdf('E:\\\\data\\\\ERA5.daily.1x1\\\\'+var+'\\\\'+var+'.'+yr+mn+'.nc')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-03T01:46:13.496826500Z",
     "start_time": "2025-07-03T01:44:27.339565300Z"
    }
   },
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
